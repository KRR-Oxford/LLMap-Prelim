{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from deeponto.onto import Ontology\n",
    "from deeponto.align.bertmap import BERTMapPipeline\n",
    "from deeponto.align.evaluation import AlignmentEvaluator\n",
    "from deeponto.utils import FileUtils\n",
    "from deeponto.align.mapping import EntityMapping, ReferenceMapping\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load source and target ontologies\n",
    "src_onto_path = \"data/ncit2doid/ncit.owl\"\n",
    "tgt_onto_path = \"data/ncit2doid/doid.owl\"\n",
    "src_onto = Ontology(src_onto_path)\n",
    "tgt_onto = Ontology(tgt_onto_path)\n",
    "config = BERTMapPipeline.load_bertmap_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build annotation index {class_iri: class_labels}\n",
    "src_annotation_index, _ = src_onto.build_annotation_index(config.annotation_property_iris)\n",
    "tgt_annotation_index, _ = tgt_onto.build_annotation_index(config.annotation_property_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xxl\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xxl\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cands = FileUtils.read_table(\"data/ncit2doid/test_cands.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# controlling input labels because of the window size limit of T5\n",
    "def process_labels(labels, cut_off = 3):\n",
    "    labels = list(labels)\n",
    "    if len(labels) >= cut_off:\n",
    "        labels.sort(key=len, reverse=True)\n",
    "    return labels[:cut_off]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_text(src_labels, tgt_labels, src_parent_labels=None, tgt_parent_labels=None, src_child_labels=None, tgt_child_labels=None):\n",
    "    v = \"Consider two concepts, each represented by a list of associated names.\\n\\n\"\n",
    "    v += f\"Source concept: {src_labels}.\\n\"\n",
    "    if src_parent_labels:\n",
    "        v = v[:-1] # removing new line character\n",
    "        v += f\" Its parent concepts: {src_parent_labels}.\\n\"\n",
    "    if src_child_labels:\n",
    "        v = v[:-1] # removing new line character\n",
    "        v += f\" Its child concepts: {src_child_labels}.\\n\"\n",
    "    v += f\"Target concept: {tgt_labels}.\\n\\n\"\n",
    "    if tgt_parent_labels:\n",
    "        v = v[:-2]\n",
    "        v += f\" Its parent concepts: {tgt_parent_labels}.\\n\\n\"\n",
    "    if tgt_child_labels:\n",
    "        v = v[:-2]\n",
    "        v += f\" Its child concepts: {tgt_child_labels}.\\n\\n\"\n",
    "    v += \"Given these representations, can you determine if the source concept and the target concept are identical? Please answer with \\\"Yes\\\" if they are identical or \\\"No\\\" if they are not.\"\n",
    "    return v\n",
    "\n",
    "print(v_text([\"A\"], [\"B\"], [\"P_A1\", \"P_A2\"], [\"P_B1\"], [\"C_A1\"], [\"C_B1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vanila setting (no structural context)\n",
    "results = []\n",
    "corrects = []\n",
    "for i, dp in test_cands.iterrows():\n",
    "    src_iri = dp[\"SrcEntity\"]\n",
    "    tgt_iri = dp[\"TgtEntity\"]\n",
    "    tgt_cands = eval(dp[\"TgtCandidates\"])\n",
    "    src_labels = process_labels(src_annotation_index[src_iri])\n",
    "    preds = []\n",
    "    scores = []\n",
    "    for tgt_cand_iri in tgt_cands:\n",
    "        tgt_cand_labels = process_labels(tgt_annotation_index[tgt_cand_iri])\n",
    "        input_ids = tokenizer(v_text(src_labels, tgt_cand_labels), return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "        outputs = model.generate(input_ids, max_new_tokens=3, return_dict_in_generate=True, output_scores=True)\n",
    "        transition_scores = model.compute_transition_scores(\n",
    "            outputs.sequences, outputs.scores, normalize_logits=True\n",
    "        )\n",
    "        input_length = 1 if model.config.is_encoder_decoder else input_ids.shape[1]\n",
    "        generated_tokens = outputs.sequences[:, input_length:]\n",
    "        has_answer = False\n",
    "        for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "            # | token | token string | logits | probability\n",
    "            score = np.exp(score.cpu().numpy())\n",
    "            tok = tokenizer.decode(tok)\n",
    "            # probs.append((tok, score))\n",
    "            if \"Yes\" in tok:\n",
    "                # print(f\"| {tok:8s} | {score:.2%}\")\n",
    "                if score > 0.55:\n",
    "                    preds.append(tgt_cand_iri)\n",
    "                scores.append((tgt_cand_iri, \"Yes\", score))\n",
    "                has_answer = True\n",
    "                break\n",
    "            if \"No\" in tok:\n",
    "                # print(f\"| {tok:8s} | {score:.2%}\")\n",
    "                scores.append((tgt_cand_iri, \"No\", score))\n",
    "                has_answer = True\n",
    "                break\n",
    "            if not has_answer:\n",
    "                # if no yes or no, giving the worst score\n",
    "                scores.append((tgt_cand_iri, \"No\", 1.0))\n",
    "    if not preds:\n",
    "        preds += [\"UnMatched\"]\n",
    "    correct = tgt_iri in preds\n",
    "    corrects.append(correct)\n",
    "    results.append(scores)\n",
    "    print(correct, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking(scores):\n",
    "    # rank the \"yes\" candidates according to their scores\n",
    "    # the \"yes\" candidates is always before the \"no\" candidates\n",
    "    # then rank the \"no\" candidates according to 1.0 - their scores\n",
    "    yes = []\n",
    "    no = []\n",
    "    for tgt_iri, answer, score in scores:\n",
    "        if answer == \"Yes\":\n",
    "            yes.append((tgt_iri, \"Yes\", score))\n",
    "        else:\n",
    "            no.append((tgt_iri, \"No\", 1.0 - score))\n",
    "    yes = list(sorted(yes, key=lambda x: x[2], reverse=True))\n",
    "    no = list(sorted(no, key=lambda x: x[2], reverse=True))\n",
    "    return yes + no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment when saving results\n",
    "# results_dict = dict()\n",
    "# for i, scores in enumerate(results):\n",
    "#     results_dict[test_cands.iloc[i][\"SrcEntity\"], test_cands.iloc[i][\"TgtEntity\"]] = ranking(scores)\n",
    "\n",
    "# FileUtils.save_file(results_dict, \"flan_t5_ncit2doid_results.pkl\")\n",
    "ranked_results = FileUtils.load_file(\"flan_t5_ncit2doid_results.pkl\")\n",
    "# the first 50 mappings are the matched mappings\n",
    "refs = ReferenceMapping.read_table_mappings(\"data/ncit2doid/refs/test_refs.tsv\")[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def mean_reciprocal_rank(prediction_and_candidates):\n",
    "    r\"\"\"Compute $MRR$ for a list of `(prediction_mapping, candidate_mappings)` pair.\n",
    "\n",
    "    $$MRR = \\sum_i^N rank_i^{-1} / N$$\n",
    "    \"\"\"\n",
    "    sum_inverted_ranks = 0\n",
    "    for pred, cands in prediction_and_candidates:\n",
    "        ordered_candidates = [c.to_tuple() for c in EntityMapping.sort_entity_mappings_by_score(cands)]\n",
    "        if pred.to_tuple() in ordered_candidates:\n",
    "            rank = ordered_candidates.index(pred.to_tuple()) + 1\n",
    "        else:\n",
    "            rank = math.inf\n",
    "        sum_inverted_ranks += 1 / rank\n",
    "    return sum_inverted_ranks / len(prediction_and_candidates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'P': 0.736, 'R': 0.78, 'F1': 0.757}\n",
      "39 39 78\n",
      "0.9533333333333335\n"
     ]
    }
   ],
   "source": [
    "# vanilla top1\n",
    "\n",
    "# Precision, Recall, F1\n",
    "preds = []\n",
    "for (src, tgt), tgt_cands in ranked_results.items():\n",
    "    t, answer, score = tgt_cands[0]\n",
    "    mapping = EntityMapping(src, t, \"=\", score)\n",
    "    if answer == \"Yes\":\n",
    "        preds.append(mapping)\n",
    "print(AlignmentEvaluator.f1(preds, refs, []))\n",
    "\n",
    "# Accuracy\n",
    "yes_correct = 0\n",
    "no_correct = 0\n",
    "for (src, tgt), tgt_cands in ranked_results.items():\n",
    "    t, answer, score = tgt_cands[0]\n",
    "    # print(tgt_cands[0])\n",
    "    if answer == \"Yes\" and tgt == t:\n",
    "        yes_correct += 1\n",
    "    elif answer == \"No\" and tgt == \"UnMatched\":\n",
    "        no_correct += 1\n",
    "print(yes_correct, no_correct, yes_correct + no_correct)\n",
    "\n",
    "# MRR\n",
    "formatted_results = []\n",
    "for (src, tgt), tgt_cands in ranked_results.items():\n",
    "    ref_mapping = EntityMapping(src, tgt, \"=\", 1.0)\n",
    "    cand_mappings = [EntityMapping(src, t, \"=\", score) for t, _, score in tgt_cands]\n",
    "    formatted_results.append((ref_mapping, cand_mappings))\n",
    "# again, only the first 50 has a match\n",
    "print(mean_reciprocal_rank(formatted_results[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'P': 0.844, 'R': 0.76, 'F1': 0.8}\n",
      "38 39 77\n",
      "0.9533333333333335\n"
     ]
    }
   ],
   "source": [
    "# compute Precision, Recall\n",
    "# vanilla top1 + threshold\n",
    "preds = []\n",
    "for (src, tgt), tgt_cands in ranked_results.items():\n",
    "    t, answer, score = tgt_cands[0]\n",
    "    mapping = EntityMapping(src, t, \"=\", score)\n",
    "    if answer == \"Yes\":\n",
    "        if score > 0.65:\n",
    "            preds.append(mapping)\n",
    "print(AlignmentEvaluator.f1(preds, refs, []))\n",
    "\n",
    "# Accuracy\n",
    "yes_correct = 0\n",
    "no_correct = 0\n",
    "for (src, tgt), tgt_cands in ranked_results.items():\n",
    "    t, answer, score = tgt_cands[0]\n",
    "    # print(tgt_cands[0])\n",
    "    if answer == \"Yes\" and tgt == t and score > 0.65:\n",
    "        yes_correct += 1\n",
    "    elif answer == \"No\" and tgt == \"UnMatched\":\n",
    "        no_correct += 1\n",
    "print(yes_correct, no_correct, yes_correct + no_correct)\n",
    "\n",
    "# MRR\n",
    "formatted_results = []\n",
    "for (src, tgt), tgt_cands in ranked_results.items():\n",
    "    ref_mapping = EntityMapping(src, tgt, \"=\", 1.0)\n",
    "    cand_mappings = [EntityMapping(src, t, \"=\", score) for t, _, score in tgt_cands]\n",
    "    formatted_results.append((ref_mapping, cand_mappings))\n",
    "# again, only the first 50 has a match\n",
    "print(mean_reciprocal_rank(formatted_results[:50]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the model again with structural context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parent_labels(ontology: Ontology, annotation_index, class_iri, cutoff = 3):\n",
    "    concept = ontology.get_owl_object_from_iri(class_iri)\n",
    "    concept_parent_iris = ontology.reasoner.get_inferred_super_entities(concept, direct=True)\n",
    "    # concept_parent_iris = [str(p.getIRI()) for p in concept_parents]\n",
    "    concept_parent_labels = []\n",
    "    for p_iri in concept_parent_iris:\n",
    "        concept_parent_labels += process_labels(annotation_index[p_iri])\n",
    "    concept_parent_labels = set(concept_parent_labels)\n",
    "    if len(concept_parent_labels) > cutoff:\n",
    "        return list(random.sample(concept_parent_labels, k=cutoff))\n",
    "    else:\n",
    "        return list(concept_parent_labels)\n",
    "    \n",
    "def get_child_labels(ontology, annotation_index, class_iri, cutoff = 3):\n",
    "    concept = ontology.get_owl_object_from_iri(class_iri)\n",
    "    concept_children_iris = ontology.reasoner.get_inferred_sub_entities(concept, direct=True)\n",
    "    # concept_children_iris = [str(p.getIRI()) for p in concept_children]\n",
    "    concept_children_labels = []\n",
    "    for p_iri in concept_children_iris:\n",
    "        concept_children_labels += process_labels(annotation_index[p_iri])\n",
    "    concept_children_labels = set(concept_children_labels)\n",
    "    if len(concept_children_labels) > cutoff:\n",
    "        return list(random.sample(concept_children_labels, k=cutoff))\n",
    "    else:\n",
    "        return list(concept_children_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vanila setting (no structural context)\n",
    "results = []\n",
    "corrects = []\n",
    "for i, dp in test_cands.iterrows():\n",
    "    src_iri = dp[\"SrcEntity\"]\n",
    "    src_parents = get_parent_labels(src_onto, src_annotation_index, src_iri)\n",
    "    src_children = get_child_labels(src_onto, src_annotation_index, src_iri)\n",
    "    tgt_iri = dp[\"TgtEntity\"]\n",
    "    tgt_cands = eval(dp[\"TgtCandidates\"])\n",
    "    src_labels = process_labels(src_annotation_index[src_iri])\n",
    "    preds = []\n",
    "    scores = []\n",
    "    for tgt_cand_iri in tgt_cands:\n",
    "        tgt_cand_labels = process_labels(tgt_annotation_index[tgt_cand_iri])\n",
    "        tgt_cand_parents = get_parent_labels(tgt_onto, tgt_annotation_index, tgt_cand_iri)\n",
    "        tgt_cand_children = get_child_labels(tgt_onto, tgt_annotation_index, tgt_cand_iri)\n",
    "        # print(v_text(src_labels, tgt_cand_labels, src_parents, tgt_cand_parents, src_children, tgt_cand_children))\n",
    "        # continue\n",
    "        input_ids = tokenizer(v_text(src_labels, tgt_cand_labels, src_parents, tgt_cand_parents, src_children, tgt_cand_children), return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "        outputs = model.generate(input_ids, max_new_tokens=3, return_dict_in_generate=True, output_scores=True)\n",
    "        transition_scores = model.compute_transition_scores(\n",
    "            outputs.sequences, outputs.scores, normalize_logits=True\n",
    "        )\n",
    "        input_length = 1 if model.config.is_encoder_decoder else input_ids.shape[1]\n",
    "        generated_tokens = outputs.sequences[:, input_length:]\n",
    "        has_answer = False\n",
    "        for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "            # | token | token string | logits | probability\n",
    "            score = np.exp(score.cpu().numpy())\n",
    "            tok = tokenizer.decode(tok)\n",
    "            # probs.append((tok, score))\n",
    "            if \"Yes\" in tok:\n",
    "                # print(f\"| {tok:8s} | {score:.2%}\")\n",
    "                if score > 0.55:\n",
    "                    preds.append(tgt_cand_iri)\n",
    "                scores.append((tgt_cand_iri, \"Yes\", score))\n",
    "                has_answer = True\n",
    "                break\n",
    "            if \"No\" in tok:\n",
    "                # print(f\"| {tok:8s} | {score:.2%}\")\n",
    "                scores.append((tgt_cand_iri, \"No\", score))\n",
    "                has_answer = True\n",
    "                break\n",
    "            if not has_answer:\n",
    "                # if no yes or no, giving the worst score\n",
    "                scores.append((tgt_cand_iri, \"No\", 1.0))\n",
    "    if not preds:\n",
    "        preds += [\"UnMatched\"]\n",
    "    correct = tgt_iri in preds\n",
    "    corrects.append(correct)\n",
    "    results.append(scores)\n",
    "    print(correct, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_dict = dict()\n",
    "# for i, scores in enumerate(results):\n",
    "#     results_dict[test_cands.iloc[i][\"SrcEntity\"], test_cands.iloc[i][\"TgtEntity\"]] = ranking(scores)\n",
    "\n",
    "# FileUtils.save_file(results_dict, \"flan_t5_ncit2doid_results_context.pkl\")\n",
    "ranked_results = FileUtils.load_file(\"flan_t5_ncit2doid_results_context.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'P': 0.789, 'R': 0.6, 'F1': 0.682}\n",
      "30 45 75\n",
      "0.9416666666666668\n"
     ]
    }
   ],
   "source": [
    "# compute Precision, Recall\n",
    "preds = []\n",
    "for (src, tgt), tgt_cands in ranked_results.items():\n",
    "    t, answer, score = tgt_cands[0]\n",
    "    mapping = EntityMapping(src, t, \"=\", score)\n",
    "    if answer == \"Yes\":\n",
    "        preds.append(mapping)\n",
    "print(AlignmentEvaluator.f1(preds, refs, []))\n",
    "\n",
    "# Accuracy\n",
    "yes_correct = 0\n",
    "no_correct = 0\n",
    "for (src, tgt), tgt_cands in ranked_results.items():\n",
    "    t, answer, score = tgt_cands[0]\n",
    "    # print(tgt_cands[0])\n",
    "    if answer == \"Yes\" and tgt == t:\n",
    "        yes_correct += 1\n",
    "    elif answer == \"No\" and tgt == \"UnMatched\":\n",
    "        no_correct += 1\n",
    "print(yes_correct, no_correct, yes_correct + no_correct)\n",
    "\n",
    "# MRR\n",
    "formatted_results = []\n",
    "for (src, tgt), tgt_cands in ranked_results.items():\n",
    "    ref_mapping = EntityMapping(src, tgt, \"=\", 1.0)\n",
    "    cand_mappings = [EntityMapping(src, t, \"=\", score) for t, _, score in tgt_cands]\n",
    "    formatted_results.append((ref_mapping, cand_mappings))\n",
    "# again, only the first 50 has a match\n",
    "print(mean_reciprocal_rank(formatted_results[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'P': 0.909, 'R': 0.4, 'F1': 0.556}\n",
      "20 45 65\n",
      "0.9416666666666668\n"
     ]
    }
   ],
   "source": [
    "# compute Precision, Recall\n",
    "preds = []\n",
    "for (src, tgt), tgt_cands in ranked_results.items():\n",
    "    t, answer, score = tgt_cands[0]\n",
    "    mapping = EntityMapping(src, t, \"=\", score)\n",
    "    if answer == \"Yes\":\n",
    "        if score > 0.65:\n",
    "            preds.append(mapping)\n",
    "print(AlignmentEvaluator.f1(preds, refs, []))\n",
    "\n",
    "# Accuracy\n",
    "yes_correct = 0\n",
    "no_correct = 0\n",
    "for (src, tgt), tgt_cands in ranked_results.items():\n",
    "    t, answer, score = tgt_cands[0]\n",
    "    # print(tgt_cands[0])\n",
    "    if answer == \"Yes\" and tgt == t and score > 0.65:\n",
    "        yes_correct += 1\n",
    "    elif answer == \"No\" and tgt == \"UnMatched\":\n",
    "        no_correct += 1\n",
    "print(yes_correct, no_correct, yes_correct + no_correct)\n",
    "\n",
    "# MRR\n",
    "formatted_results = []\n",
    "for (src, tgt), tgt_cands in ranked_results.items():\n",
    "    ref_mapping = EntityMapping(src, tgt, \"=\", 1.0)\n",
    "    cand_mappings = [EntityMapping(src, t, \"=\", score) for t, _, score in tgt_cands]\n",
    "    formatted_results.append((ref_mapping, cand_mappings))\n",
    "# again, only the first 50 has a match\n",
    "print(mean_reciprocal_rank(formatted_results[:50]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeponto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
