{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the maximum memory located to JVM [8g]:\n",
      "8g maximum memory allocated to JVM.\n",
      "JVM started successfully.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from deeponto.onto import Ontology\n",
    "from deeponto.align.bertmap import BERTMapPipeline\n",
    "from deeponto.align.evaluation import AlignmentEvaluator\n",
    "from deeponto.utils import FileUtils\n",
    "from deeponto.align.mapping import EntityMapping, ReferenceMapping\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[main] WARN uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl - Illegal redeclarations of entities: reuse of entity http://purl.org/sig/ont/fma/has_direct_shape_type in punning not allowed [Declaration(DataProperty(<http://purl.org/sig/ont/fma/has_direct_shape_type>)), Declaration(ObjectProperty(<http://purl.org/sig/ont/fma/has_direct_shape_type>))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the default configuration at /home/yuan/anaconda3/envs/deeponto/lib/python3.8/site-packages/deeponto/align/bertmap/default_config.yaml.\n"
     ]
    }
   ],
   "source": [
    "# load source and target ontologies\n",
    "src_onto_path = \"data/snomed2fma/snomed.body.owl\"\n",
    "tgt_onto_path = \"data/snomed2fma/fma.body.owl\"\n",
    "src_onto = Ontology(src_onto_path)\n",
    "tgt_onto = Ontology(tgt_onto_path)\n",
    "config = BERTMapPipeline.load_bertmap_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build annotation index {class_iri: class_labels}\n",
    "src_annotation_index, _ = src_onto.build_annotation_index(config.annotation_property_iris)\n",
    "tgt_annotation_index, _ = tgt_onto.build_annotation_index(config.annotation_property_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656f303a77564246818d8e45fb610c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xxl\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xxl\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cands = FileUtils.read_table(\"data/snomed2fma/test_cands.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# controlling input labels because of the window size limit of T5\n",
    "def process_labels(labels, cut_off = 3):\n",
    "    labels = list(labels)\n",
    "    if len(labels) >= cut_off:\n",
    "        labels.sort(key=len, reverse=True)\n",
    "    return labels[:cut_off]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider two concepts, each represented by a list of names, and associated with additional contexts.\n",
      "\n",
      "Source Concept:\n",
      "Names: ['A']\n",
      "Parent Concepts: ['P_A1', 'P_A2']\n",
      "Child Concepts: ['C_A1']\n",
      "\n",
      "Target Concept:\n",
      "Names: ['B']\n",
      "Parent Concepts: ['P_B1']\n",
      "Child Concepts: ['C_B1']\n",
      "\n",
      "Based on the provided names, parent and child concepts (if any), determine if the Source Concept and the Target Concept are identical. Please respond with either 'Yes' (if they are identical) or 'No' (if they are not identical).\n"
     ]
    }
   ],
   "source": [
    "def v_text(src_labels, tgt_labels, src_parent_labels=None, tgt_parent_labels=None, src_child_labels=None, tgt_child_labels=None):\n",
    "    v = f\"Source Concept:\\n\"\n",
    "    v += f\"Names: {src_labels}\\n\"\n",
    "    has_parent_child = False\n",
    "    if src_parent_labels:\n",
    "        v += f\"Parent Concepts: {src_parent_labels}\\n\"\n",
    "        has_parent_child = True\n",
    "    if src_child_labels:\n",
    "        v += f\"Child Concepts: {src_child_labels}\\n\"\n",
    "        has_parent_child = True\n",
    "    v +=\"\\n\"\n",
    "    v += f\"Target Concept:\\n\"\n",
    "    v += f\"Names: {tgt_labels}\\n\"\n",
    "    if tgt_parent_labels:\n",
    "        v += f\"Parent Concepts: {tgt_parent_labels}\\n\"\n",
    "        has_parent_child = True\n",
    "    if tgt_child_labels:\n",
    "        v += f\"Child Concepts: {tgt_child_labels}\\n\"\n",
    "        has_parent_child = True\n",
    "    v +=\"\\n\"\n",
    "    if not has_parent_child:\n",
    "        v = \"Consider two concepts, each represented by a list of names.\\n\\n\" + v\n",
    "        v += \"Based on the provided names, determine if the Source Concept and the Target Concept are identical. Please respond with either 'Yes' (if they are identical) or 'No' (if they are not identical).\"\n",
    "    else:\n",
    "        v = \"Consider two concepts, each represented by a list of names, and associated with additional contexts.\\n\\n\" + v\n",
    "        v += \"Based on the provided names, parent and child concepts (if any), determine if the Source Concept and the Target Concept are identical. Please respond with either 'Yes' (if they are identical) or 'No' (if they are not identical).\"\n",
    "    return v\n",
    "\n",
    "print(v_text([\"A\"], [\"B\"], [\"P_A1\", \"P_A2\"], [\"P_B1\"], [\"C_A1\"], [\"C_B1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True ['http://purl.org/sig/ont/fma/fma16072']\n",
      "True ['http://purl.org/sig/ont/fma/fma83945']\n",
      "False ['UnMatched']\n",
      "True ['http://purl.org/sig/ont/fma/fma71410']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "True ['http://purl.org/sig/ont/fma/fma47200']\n",
      "True ['http://purl.org/sig/ont/fma/fma6652']\n",
      "True ['http://purl.org/sig/ont/fma/fma323351']\n",
      "False ['UnMatched']\n",
      "True ['http://purl.org/sig/ont/fma/fma71766', 'http://purl.org/sig/ont/fma/fma5950']\n",
      "True ['http://purl.org/sig/ont/fma/fma8661']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "True ['http://purl.org/sig/ont/fma/fma61020']\n",
      "False ['UnMatched']\n",
      "True ['http://purl.org/sig/ont/fma/fma75189', 'http://purl.org/sig/ont/fma/fma19618', 'http://purl.org/sig/ont/fma/fma20281']\n",
      "False ['http://purl.org/sig/ont/fma/fma75189', 'http://purl.org/sig/ont/fma/fma19618', 'http://purl.org/sig/ont/fma/fma20281']\n",
      "True ['http://purl.org/sig/ont/fma/fma13715', 'http://purl.org/sig/ont/fma/fma13707']\n",
      "True ['http://purl.org/sig/ont/fma/fma24139']\n",
      "True ['http://purl.org/sig/ont/fma/fma72617']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "True ['http://purl.org/sig/ont/fma/fma51335', 'http://purl.org/sig/ont/fma/fma71572']\n",
      "True ['http://purl.org/sig/ont/fma/fma51335', 'http://purl.org/sig/ont/fma/fma71572']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "True ['http://purl.org/sig/ont/fma/fma32699', 'http://purl.org/sig/ont/fma/fma34360']\n",
      "False ['http://purl.org/sig/ont/fma/fma15767']\n",
      "True ['http://purl.org/sig/ont/fma/fma4859']\n",
      "True ['http://purl.org/sig/ont/fma/fma52571']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "True ['http://purl.org/sig/ont/fma/fma61021']\n",
      "False ['UnMatched']\n",
      "True ['http://purl.org/sig/ont/fma/fma38673']\n",
      "True ['http://purl.org/sig/ont/fma/fma6339']\n",
      "False ['http://purl.org/sig/ont/fma/fma293451', 'http://purl.org/sig/ont/fma/fma59796']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "True ['http://purl.org/sig/ont/fma/fma49621']\n",
      "False ['UnMatched']\n",
      "False ['http://purl.org/sig/ont/fma/fma14186']\n",
      "False ['http://purl.org/sig/ont/fma/fma14186']\n",
      "False ['http://purl.org/sig/ont/fma/fma14186']\n",
      "True ['http://purl.org/sig/ont/fma/fma75547', 'http://purl.org/sig/ont/fma/fma75549', 'http://purl.org/sig/ont/fma/fma52679']\n",
      "True ['http://purl.org/sig/ont/fma/fma75499']\n",
      "True ['http://purl.org/sig/ont/fma/fma15379']\n",
      "False ['http://purl.org/sig/ont/fma/fma75509']\n",
      "True ['http://purl.org/sig/ont/fma/fma75509']\n",
      "False ['http://purl.org/sig/ont/fma/fma6474']\n",
      "False ['UnMatched']\n",
      "False ['http://purl.org/sig/ont/fma/fma15843']\n",
      "False ['http://purl.org/sig/ont/fma/fma15843']\n",
      "False ['UnMatched']\n",
      "True ['UnMatched']\n",
      "False ['http://purl.org/sig/ont/fma/fma7200']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "False ['http://purl.org/sig/ont/fma/fma51095']\n",
      "False ['http://purl.org/sig/ont/fma/fma37849']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "False ['http://purl.org/sig/ont/fma/fma46583']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "False ['http://purl.org/sig/ont/fma/fma16180']\n",
      "False ['http://purl.org/sig/ont/fma/fma53642']\n",
      "False ['http://purl.org/sig/ont/fma/fma37852', 'http://purl.org/sig/ont/fma/fma322625']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "False ['http://purl.org/sig/ont/fma/fma43056']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "False ['http://purl.org/sig/ont/fma/fma322937']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "False ['http://purl.org/sig/ont/fma/fma48159', 'http://purl.org/sig/ont/fma/fma48160']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n"
     ]
    }
   ],
   "source": [
    "# vanila setting (no structural context)\n",
    "results = []\n",
    "corrects = []\n",
    "for i, dp in test_cands.iterrows():\n",
    "    src_iri = dp[\"SrcEntity\"]\n",
    "    tgt_iri = dp[\"TgtEntity\"]\n",
    "    tgt_cands = eval(dp[\"TgtCandidates\"])\n",
    "    src_labels = process_labels(src_annotation_index[src_iri])\n",
    "    preds = []\n",
    "    scores = []\n",
    "    for tgt_cand_iri in tgt_cands:\n",
    "        tgt_cand_labels = process_labels(tgt_annotation_index[tgt_cand_iri])\n",
    "        input_ids = tokenizer(v_text(src_labels, tgt_cand_labels), return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "        outputs = model.generate(input_ids, max_new_tokens=3, return_dict_in_generate=True, output_scores=True)\n",
    "        transition_scores = model.compute_transition_scores(\n",
    "            outputs.sequences, outputs.scores, normalize_logits=True\n",
    "        )\n",
    "        input_length = 1 if model.config.is_encoder_decoder else input_ids.shape[1]\n",
    "        generated_tokens = outputs.sequences[:, input_length:]\n",
    "        has_answer = False\n",
    "        for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "            # | token | token string | logits | probability\n",
    "            score = np.exp(score.cpu().numpy())\n",
    "            tok = tokenizer.decode(tok)\n",
    "            # probs.append((tok, score))\n",
    "            if \"Yes\" in tok:\n",
    "                # print(f\"| {tok:8s} | {score:.2%}\")\n",
    "                if score > 0.55:\n",
    "                    preds.append(tgt_cand_iri)\n",
    "                scores.append((tgt_cand_iri, \"Yes\", score))\n",
    "                has_answer = True\n",
    "                break\n",
    "            if \"No\" in tok:\n",
    "                # print(f\"| {tok:8s} | {score:.2%}\")\n",
    "                scores.append((tgt_cand_iri, \"No\", score))\n",
    "                has_answer = True\n",
    "                break\n",
    "            if not has_answer:\n",
    "                # if no yes or no, giving the worst score\n",
    "                scores.append((tgt_cand_iri, \"No\", 1.0))\n",
    "    if not preds:\n",
    "        preds += [\"UnMatched\"]\n",
    "    correct = tgt_iri in preds\n",
    "    corrects.append(correct)\n",
    "    results.append(scores)\n",
    "    print(correct, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking(scores):\n",
    "    # rank the \"yes\" candidates according to their scores\n",
    "    # the \"yes\" candidates is always before the \"no\" candidates\n",
    "    # then rank the \"no\" candidates according to 1.0 - their scores\n",
    "    yes = []\n",
    "    no = []\n",
    "    for tgt_iri, answer, score in scores:\n",
    "        if answer == \"Yes\":\n",
    "            yes.append((tgt_iri, \"Yes\", score))\n",
    "        else:\n",
    "            no.append((tgt_iri, \"No\", 1.0 - score))\n",
    "    yes = list(sorted(yes, key=lambda x: x[2], reverse=True))\n",
    "    no = list(sorted(no, key=lambda x: x[2], reverse=True))\n",
    "    return yes + no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment when saving results\n",
    "results_dict = dict()\n",
    "for i, scores in enumerate(results):\n",
    "    results_dict[test_cands.iloc[i][\"SrcEntity\"], test_cands.iloc[i][\"TgtEntity\"]] = ranking(scores)\n",
    "\n",
    "FileUtils.save_file(results_dict, \"flan_t5_snomed2fma_results.pkl\")\n",
    "ranked_results = FileUtils.load_file(\"flan_t5_snomed2fma_results.pkl\")\n",
    "# the first 50 mappings are the matched mappings\n",
    "refs = ReferenceMapping.read_table_mappings(\"data/snomed2fma/refs/test_refs.tsv\")[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def mean_reciprocal_rank(prediction_and_candidates):\n",
    "    r\"\"\"Compute $MRR$ for a list of `(prediction_mapping, candidate_mappings)` pair.\n",
    "\n",
    "    $$MRR = \\sum_i^N rank_i^{-1} / N$$\n",
    "    \"\"\"\n",
    "    sum_inverted_ranks = 0\n",
    "    for pred, cands in prediction_and_candidates:\n",
    "        ordered_candidates = [c.to_tuple() for c in EntityMapping.sort_entity_mappings_by_score(cands)]\n",
    "        if pred.to_tuple() in ordered_candidates:\n",
    "            rank = ordered_candidates.index(pred.to_tuple()) + 1\n",
    "        else:\n",
    "            rank = math.inf\n",
    "        sum_inverted_ranks += 1 / rank\n",
    "    return sum_inverted_ranks / len(prediction_and_candidates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'P': 0.479, 'R': 0.46, 'F1': 0.469}\n",
      "26 35 61\n",
      "0.8011904761904763\n"
     ]
    }
   ],
   "source": [
    "# vanilla top1\n",
    "\n",
    "# Precision, Recall, F1\n",
    "preds = []\n",
    "for (src, tgt), tgt_cands in ranked_results.items():\n",
    "    t, answer, score = tgt_cands[0]\n",
    "    mapping = EntityMapping(src, t, \"=\", score)\n",
    "    if answer == \"Yes\":\n",
    "        preds.append(mapping)\n",
    "print(AlignmentEvaluator.f1(preds, refs, []))\n",
    "\n",
    "# Accuracy\n",
    "yes_correct = 0\n",
    "no_correct = 0\n",
    "for (src, tgt), tgt_cands in ranked_results.items():\n",
    "    t, answer, score = tgt_cands[0]\n",
    "    # print(tgt_cands[0])\n",
    "    if answer == \"Yes\" and tgt == t:\n",
    "        yes_correct += 1\n",
    "    elif answer == \"No\" and tgt == \"UnMatched\":\n",
    "        no_correct += 1\n",
    "print(yes_correct, no_correct, yes_correct + no_correct)\n",
    "\n",
    "# MRR\n",
    "formatted_results = []\n",
    "for (src, tgt), tgt_cands in ranked_results.items():\n",
    "    ref_mapping = EntityMapping(src, tgt, \"=\", 1.0)\n",
    "    cand_mappings = [EntityMapping(src, t, \"=\", score) for t, _, score in tgt_cands]\n",
    "    formatted_results.append((ref_mapping, cand_mappings))\n",
    "# again, only the first 50 has a match\n",
    "print(mean_reciprocal_rank(formatted_results[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'P': 0.5, 'R': 0.4, 'F1': 0.444}\n",
      "23 35 58\n",
      "0.8011904761904763\n"
     ]
    }
   ],
   "source": [
    "# compute Precision, Recall\n",
    "# vanilla top1 + threshold\n",
    "preds = []\n",
    "for (src, tgt), tgt_cands in ranked_results.items():\n",
    "    t, answer, score = tgt_cands[0]\n",
    "    mapping = EntityMapping(src, t, \"=\", score)\n",
    "    if answer == \"Yes\":\n",
    "        if score > 0.55:\n",
    "            preds.append(mapping)\n",
    "print(AlignmentEvaluator.f1(preds, refs, []))\n",
    "\n",
    "# Accuracy\n",
    "yes_correct = 0\n",
    "no_correct = 0\n",
    "for (src, tgt), tgt_cands in ranked_results.items():\n",
    "    t, answer, score = tgt_cands[0]\n",
    "    # print(tgt_cands[0])\n",
    "    if answer == \"Yes\" and tgt == t and score > 0.55:\n",
    "        yes_correct += 1\n",
    "    elif answer == \"No\" and tgt == \"UnMatched\":\n",
    "        no_correct += 1\n",
    "print(yes_correct, no_correct, yes_correct + no_correct)\n",
    "\n",
    "# MRR\n",
    "formatted_results = []\n",
    "for (src, tgt), tgt_cands in ranked_results.items():\n",
    "    ref_mapping = EntityMapping(src, tgt, \"=\", 1.0)\n",
    "    cand_mappings = [EntityMapping(src, t, \"=\", score) for t, _, score in tgt_cands]\n",
    "    formatted_results.append((ref_mapping, cand_mappings))\n",
    "# again, only the first 50 has a match\n",
    "print(mean_reciprocal_rank(formatted_results[:50]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the model again with structural context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parent_labels(ontology: Ontology, annotation_index, class_iri, cutoff = 3):\n",
    "    concept = ontology.get_owl_object_from_iri(class_iri)\n",
    "    concept_parents = ontology.get_asserted_parents(concept)\n",
    "    # concept_parent_iris = [str(p.getIRI()) for p in concept_parents]\n",
    "    concept_parent_labels = []\n",
    "    for p in concept_parents:\n",
    "        try:\n",
    "            p_iri = str(p.getIRI())\n",
    "            concept_parent_labels += process_labels(annotation_index[p_iri])\n",
    "        except:\n",
    "            continue\n",
    "    concept_parent_labels = set(concept_parent_labels)\n",
    "    if len(concept_parent_labels) > cutoff:\n",
    "        return list(random.sample(concept_parent_labels, k=cutoff))\n",
    "    else:\n",
    "        return list(concept_parent_labels)\n",
    "    \n",
    "def get_child_labels(ontology, annotation_index, class_iri, cutoff = 3):\n",
    "    concept = ontology.get_owl_object_from_iri(class_iri)\n",
    "    concept_children = ontology.get_asserted_children(concept)\n",
    "    # concept_children_iris = [str(p.getIRI()) for p in concept_children]\n",
    "    concept_children_labels = []\n",
    "    for c in concept_children:\n",
    "        try: \n",
    "            c_iri = str(c.getIRI())\n",
    "            concept_children_labels += process_labels(annotation_index[c_iri])\n",
    "        except:\n",
    "            continue\n",
    "    concept_children_labels = set(concept_children_labels)\n",
    "    if len(concept_children_labels) > cutoff:\n",
    "        return list(random.sample(concept_children_labels, k=cutoff))\n",
    "    else:\n",
    "        return list(concept_children_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True ['http://purl.org/sig/ont/fma/fma16072']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "True ['http://purl.org/sig/ont/fma/fma47200']\n",
      "False ['UnMatched']\n",
      "True ['http://purl.org/sig/ont/fma/fma323351']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "True ['http://purl.org/sig/ont/fma/fma8661']\n",
      "False ['UnMatched']\n",
      "True ['http://purl.org/sig/ont/fma/fma84245']\n",
      "True ['http://purl.org/sig/ont/fma/fma61020']\n",
      "False ['UnMatched']\n",
      "True ['http://purl.org/sig/ont/fma/fma75189', 'http://purl.org/sig/ont/fma/fma19618']\n",
      "False ['http://purl.org/sig/ont/fma/fma75189', 'http://purl.org/sig/ont/fma/fma19618']\n",
      "True ['http://purl.org/sig/ont/fma/fma13715']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "False ['http://purl.org/sig/ont/fma/fma15767']\n",
      "True ['http://purl.org/sig/ont/fma/fma4859']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "True ['http://purl.org/sig/ont/fma/fma61021']\n",
      "False ['UnMatched']\n",
      "True ['http://purl.org/sig/ont/fma/fma38673']\n",
      "False ['UnMatched']\n",
      "False ['http://purl.org/sig/ont/fma/fma293451', 'http://purl.org/sig/ont/fma/fma59796']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "True ['http://purl.org/sig/ont/fma/fma49621']\n",
      "False ['UnMatched']\n",
      "False ['http://purl.org/sig/ont/fma/fma14186']\n",
      "False ['http://purl.org/sig/ont/fma/fma14186']\n",
      "False ['http://purl.org/sig/ont/fma/fma14186']\n",
      "False ['http://purl.org/sig/ont/fma/fma75547']\n",
      "True ['http://purl.org/sig/ont/fma/fma75499']\n",
      "True ['http://purl.org/sig/ont/fma/fma15379']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "False ['UnMatched']\n",
      "False ['http://purl.org/sig/ont/fma/fma15843']\n",
      "False ['http://purl.org/sig/ont/fma/fma15843']\n",
      "False ['UnMatched']\n",
      "True ['UnMatched']\n",
      "False ['http://purl.org/sig/ont/fma/fma7200']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "False ['http://purl.org/sig/ont/fma/fma53642']\n",
      "False ['http://purl.org/sig/ont/fma/fma322625']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "False ['http://purl.org/sig/ont/fma/fma322937']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n",
      "True ['UnMatched']\n"
     ]
    }
   ],
   "source": [
    "# with structural context\n",
    "results = []\n",
    "corrects = []\n",
    "for i, dp in test_cands.iterrows():\n",
    "    src_iri = dp[\"SrcEntity\"]\n",
    "    src_parents = get_parent_labels(src_onto, src_annotation_index, src_iri)\n",
    "    src_children = get_child_labels(src_onto, src_annotation_index, src_iri)\n",
    "    tgt_iri = dp[\"TgtEntity\"]\n",
    "    tgt_cands = eval(dp[\"TgtCandidates\"])\n",
    "    src_labels = process_labels(src_annotation_index[src_iri])\n",
    "    preds = []\n",
    "    scores = []\n",
    "    for tgt_cand_iri in tgt_cands:\n",
    "        tgt_cand_labels = process_labels(tgt_annotation_index[tgt_cand_iri])\n",
    "        tgt_cand_parents = get_parent_labels(tgt_onto, tgt_annotation_index, tgt_cand_iri)\n",
    "        tgt_cand_children = get_child_labels(tgt_onto, tgt_annotation_index, tgt_cand_iri)\n",
    "        # print(v_text(src_labels, tgt_cand_labels, src_parents, tgt_cand_parents, src_children, tgt_cand_children))\n",
    "        # continue\n",
    "        input_ids = tokenizer(v_text(src_labels, tgt_cand_labels, src_parents, tgt_cand_parents, src_children, tgt_cand_children), return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "        outputs = model.generate(input_ids, max_new_tokens=3, return_dict_in_generate=True, output_scores=True)\n",
    "        transition_scores = model.compute_transition_scores(\n",
    "            outputs.sequences, outputs.scores, normalize_logits=True\n",
    "        )\n",
    "        input_length = 1 if model.config.is_encoder_decoder else input_ids.shape[1]\n",
    "        generated_tokens = outputs.sequences[:, input_length:]\n",
    "        has_answer = False\n",
    "        for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "            # | token | token string | logits | probability\n",
    "            score = np.exp(score.cpu().numpy())\n",
    "            tok = tokenizer.decode(tok)\n",
    "            # probs.append((tok, score))\n",
    "            if \"Yes\" in tok:\n",
    "                # print(f\"| {tok:8s} | {score:.2%}\")\n",
    "                if score > 0.55:\n",
    "                    preds.append(tgt_cand_iri)\n",
    "                scores.append((tgt_cand_iri, \"Yes\", score))\n",
    "                has_answer = True\n",
    "                break\n",
    "            if \"No\" in tok:\n",
    "                # print(f\"| {tok:8s} | {score:.2%}\")\n",
    "                scores.append((tgt_cand_iri, \"No\", score))\n",
    "                has_answer = True\n",
    "                break\n",
    "            if not has_answer:\n",
    "                # if no yes or no, giving the worst score\n",
    "                scores.append((tgt_cand_iri, \"No\", 1.0))\n",
    "    if not preds:\n",
    "        preds += [\"UnMatched\"]\n",
    "    correct = tgt_iri in preds\n",
    "    corrects.append(correct)\n",
    "    results.append(scores)\n",
    "    print(correct, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = dict()\n",
    "for i, scores in enumerate(results):\n",
    "    results_dict[test_cands.iloc[i][\"SrcEntity\"], test_cands.iloc[i][\"TgtEntity\"]] = ranking(scores)\n",
    "\n",
    "FileUtils.save_file(results_dict, \"flan_t5_snomed2fma_results_context.pkl\")\n",
    "ranked_results = FileUtils.load_file(\"flan_t5_snomed2fma_results_context.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'P': 0.5, 'R': 0.26, 'F1': 0.342}\n",
      "17 46 63\n",
      "0.7254080267558528\n"
     ]
    }
   ],
   "source": [
    "# compute Precision, Recall\n",
    "preds = []\n",
    "for (src, tgt), tgt_cands in ranked_results.items():\n",
    "    t, answer, score = tgt_cands[0]\n",
    "    mapping = EntityMapping(src, t, \"=\", score)\n",
    "    if answer == \"Yes\":\n",
    "        preds.append(mapping)\n",
    "print(AlignmentEvaluator.f1(preds, refs, []))\n",
    "\n",
    "# Accuracy\n",
    "yes_correct = 0\n",
    "no_correct = 0\n",
    "for (src, tgt), tgt_cands in ranked_results.items():\n",
    "    t, answer, score = tgt_cands[0]\n",
    "    # print(tgt_cands[0])\n",
    "    if answer == \"Yes\" and tgt == t:\n",
    "        yes_correct += 1\n",
    "    elif answer == \"No\" and tgt == \"UnMatched\":\n",
    "        no_correct += 1\n",
    "print(yes_correct, no_correct, yes_correct + no_correct)\n",
    "\n",
    "# MRR\n",
    "formatted_results = []\n",
    "for (src, tgt), tgt_cands in ranked_results.items():\n",
    "    ref_mapping = EntityMapping(src, tgt, \"=\", 1.0)\n",
    "    cand_mappings = [EntityMapping(src, t, \"=\", score) for t, _, score in tgt_cands]\n",
    "    formatted_results.append((ref_mapping, cand_mappings))\n",
    "# again, only the first 50 has a match\n",
    "print(mean_reciprocal_rank(formatted_results[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'P': 0.667, 'R': 0.2, 'F1': 0.308}\n",
      "11 46 57\n",
      "0.7254080267558528\n"
     ]
    }
   ],
   "source": [
    "# compute Precision, Recall\n",
    "preds = []\n",
    "for (src, tgt), tgt_cands in ranked_results.items():\n",
    "    t, answer, score = tgt_cands[0]\n",
    "    mapping = EntityMapping(src, t, \"=\", score)\n",
    "    if answer == \"Yes\":\n",
    "        if score > 0.65:\n",
    "            preds.append(mapping)\n",
    "print(AlignmentEvaluator.f1(preds, refs, []))\n",
    "\n",
    "# Accuracy\n",
    "yes_correct = 0\n",
    "no_correct = 0\n",
    "for (src, tgt), tgt_cands in ranked_results.items():\n",
    "    t, answer, score = tgt_cands[0]\n",
    "    # print(tgt_cands[0])\n",
    "    if answer == \"Yes\" and tgt == t and score > 0.65:\n",
    "        yes_correct += 1\n",
    "    elif answer == \"No\" and tgt == \"UnMatched\":\n",
    "        no_correct += 1\n",
    "print(yes_correct, no_correct, yes_correct + no_correct)\n",
    "\n",
    "# MRR\n",
    "formatted_results = []\n",
    "for (src, tgt), tgt_cands in ranked_results.items():\n",
    "    ref_mapping = EntityMapping(src, tgt, \"=\", 1.0)\n",
    "    cand_mappings = [EntityMapping(src, t, \"=\", score) for t, _, score in tgt_cands]\n",
    "    formatted_results.append((ref_mapping, cand_mappings))\n",
    "# again, only the first 50 has a match\n",
    "print(mean_reciprocal_rank(formatted_results[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeponto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
